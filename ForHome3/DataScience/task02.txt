
What is the problem with always using R 2 ?
R2 показва колко добре модела описва данните, но това значи, че ако ги overfit-не R2 би било високо отново.


How does using R a d j 2 help solve this problem?
Radj2 решава този проблем, като делим на броя на променливите, които използва моделът.
По този начин, ако моделът не получава съществена информация от дадена променлива,
метриката ще намалее.


How could we calculate R a d j 2 in Python?

n = X.shape[0]  # Number of observations
p = X.shape[1]  # Number of predictors
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print(f"Adjusted R-squared: {adjusted_r2:.4f}")

https://www.geeksforgeeks.org/machine-learning/r-squared-vs-adjusted-r-squared-difference/